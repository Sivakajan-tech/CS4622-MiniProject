{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas to create DataFrame \n",
    "import pandas as pd\n",
    "\n",
    "#Ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "271b7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame of the given data \n",
    "train = pd.read_csv('train.csv')\n",
    "valid = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048ea9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_763</th>\n",
       "      <th>feature_764</th>\n",
       "      <th>feature_765</th>\n",
       "      <th>feature_766</th>\n",
       "      <th>feature_767</th>\n",
       "      <th>feature_768</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.071810</td>\n",
       "      <td>0.068413</td>\n",
       "      <td>-0.022749</td>\n",
       "      <td>0.086143</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>-0.028817</td>\n",
       "      <td>0.199237</td>\n",
       "      <td>-0.287368</td>\n",
       "      <td>-0.059560</td>\n",
       "      <td>-0.043694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>0.123011</td>\n",
       "      <td>0.043040</td>\n",
       "      <td>-0.042152</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.072623</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.057811</td>\n",
       "      <td>-0.230877</td>\n",
       "      <td>-0.146281</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.128767</td>\n",
       "      <td>-0.146269</td>\n",
       "      <td>0.053893</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077742</td>\n",
       "      <td>0.081691</td>\n",
       "      <td>-0.004778</td>\n",
       "      <td>0.171727</td>\n",
       "      <td>-0.026027</td>\n",
       "      <td>0.171089</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.004626</td>\n",
       "      <td>-0.029383</td>\n",
       "      <td>-0.165376</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>-0.028142</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>-0.082088</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>-0.046272</td>\n",
       "      <td>0.027831</td>\n",
       "      <td>0.028096</td>\n",
       "      <td>0.030994</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.086241</td>\n",
       "      <td>0.129585</td>\n",
       "      <td>-0.013893</td>\n",
       "      <td>0.089885</td>\n",
       "      <td>-0.100300</td>\n",
       "      <td>-0.035184</td>\n",
       "      <td>0.240980</td>\n",
       "      <td>-0.128362</td>\n",
       "      <td>-0.072328</td>\n",
       "      <td>-0.019385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006934</td>\n",
       "      <td>-0.049213</td>\n",
       "      <td>0.078852</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>0.168815</td>\n",
       "      <td>-0.049188</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126416</td>\n",
       "      <td>0.088338</td>\n",
       "      <td>0.088307</td>\n",
       "      <td>0.020371</td>\n",
       "      <td>0.174417</td>\n",
       "      <td>-0.030560</td>\n",
       "      <td>0.181163</td>\n",
       "      <td>-0.009382</td>\n",
       "      <td>0.085396</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028883</td>\n",
       "      <td>0.110844</td>\n",
       "      <td>-0.041875</td>\n",
       "      <td>0.025686</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>-0.104945</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 772 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  feature_6  \\\n",
       "0   0.071810   0.068413  -0.022749   0.086143   0.026361  -0.028817   \n",
       "1   0.030930   0.024088   0.057811  -0.230877  -0.146281   0.102807   \n",
       "2  -0.044019  -0.004626  -0.029383  -0.165376  -0.026611  -0.028142   \n",
       "3  -0.086241   0.129585  -0.013893   0.089885  -0.100300  -0.035184   \n",
       "4   0.126416   0.088338   0.088307   0.020371   0.174417  -0.030560   \n",
       "\n",
       "   feature_7  feature_8  feature_9  feature_10  ...  feature_763  feature_764  \\\n",
       "0   0.199237  -0.287368  -0.059560   -0.043694  ...     0.004646     0.123011   \n",
       "1   0.128767  -0.146269   0.053893    0.055378  ...     0.077742     0.081691   \n",
       "2  -0.009649  -0.082088   0.018933    0.006830  ...     0.076249    -0.046272   \n",
       "3   0.240980  -0.128362  -0.072328   -0.019385  ...     0.006934    -0.049213   \n",
       "4   0.181163  -0.009382   0.085396    0.015823  ...    -0.028883     0.110844   \n",
       "\n",
       "   feature_765  feature_766  feature_767  feature_768  label_1  label_2  \\\n",
       "0     0.043040    -0.042152     0.026225     0.072623       45      NaN   \n",
       "1    -0.004778     0.171727    -0.026027     0.171089       45      NaN   \n",
       "2     0.027831     0.028096     0.030994     0.009709       45      NaN   \n",
       "3     0.078852     0.088323     0.168815    -0.049188       45      NaN   \n",
       "4    -0.041875     0.025686     0.003534    -0.104945       45      NaN   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        6  \n",
       "1        1        6  \n",
       "2        1        6  \n",
       "3        1        6  \n",
       "4        1        6  \n",
       "\n",
       "[5 rows x 772 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bdd01976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1        0\n",
       "feature_2        0\n",
       "feature_3        0\n",
       "feature_4        0\n",
       "feature_5        0\n",
       "              ... \n",
       "feature_768      0\n",
       "label_1          0\n",
       "label_2        480\n",
       "label_3          0\n",
       "label_4          0\n",
       "Length: 772, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking provided data\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d1fa593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dependent & Independent features\n",
    "y = train[[\"label_1\",\"label_2\",\"label_3\",\"label_4\"]]\n",
    "y_valid = valid[[\"label_1\",\"label_2\",\"label_3\",\"label_4\"]] \n",
    "x = train.drop(y, axis=1)\n",
    "x_valid = valid.drop(y_valid, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85cc13",
   "metadata": {},
   "source": [
    "# Model Checking & Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f7d21",
   "metadata": {},
   "source": [
    "For KNN, I used Grid Search to do hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9940ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def try_knn(train_x, train_y, vaid_x, valid_y):\n",
    "    grid_params = { 'n_neighbors' : [3,5,7,15],\n",
    "               'weights' : ['uniform','distance']}\n",
    "    gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n",
    "    g_res = gs.fit(train_x, train_y)\n",
    "    print(f\"Best Hyperparameters:\",gs.best_params_)\n",
    "    # Get the best k-NN model with the optimal hyperparameters\n",
    "    best_knn = gs.best_estimator_\n",
    "    # Evaluate the best model on the test data\n",
    "    accuracy = best_knn.score(vaid_x, valid_y)\n",
    "    print(f\"Accuracy for KNN {accuracy}\")      \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488e15b",
   "metadata": {},
   "source": [
    "Due to performance restriction of my machine, I do hyperparameter tuning manually for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "49cfe74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def try_svm(train_x, train_y, vaid_x, valid_y):\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(train_x, train_y)\n",
    "    X_valid_contiguous = np.ascontiguousarray(vaid_x)\n",
    "    y_pred = clf.predict(X_valid_contiguous)\n",
    "    print(f\"Accuracy Score of SVM with default settings (RBF - exponential kernal) \", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = svm.LinearSVC(dual=\"auto\")\n",
    "    clf.fit(train_x, train_y)\n",
    "    X_valid_contiguous = np.ascontiguousarray(vaid_x)\n",
    "    y_pred = clf.predict(X_valid_contiguous)\n",
    "    print(f\"Accuracy Score of Linear SVM (one-vs-the-rest) \", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(train_x, train_y)\n",
    "    X_valid_contiguous = np.ascontiguousarray(vaid_x)\n",
    "    y_pred = clf.predict(X_valid_contiguous)\n",
    "    print(f\"Accuracy Score of SVM with linear kernal function \", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = svm.SVC(kernel='sigmoid')\n",
    "    clf.fit(train_x, train_y)\n",
    "    X_valid_contiguous = np.ascontiguousarray(vaid_x)\n",
    "    y_pred = clf.predict(X_valid_contiguous)\n",
    "    print(f\"Accuracy Score of SVM with sigmoid kernal function \", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = svm.SVC(kernel='poly',degree =2)\n",
    "    clf.fit(train_x, train_y)\n",
    "    X_valid_contiguous = np.ascontiguousarray(vaid_x)\n",
    "    y_pred = clf.predict(X_valid_contiguous)\n",
    "    print(f\"Accuracy Score of SVM with polynomial kernal function with degree 2 \", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = svm.SVC(kernel='poly',degree =2)\n",
    "    clf.fit(train_x, train_y)\n",
    "    X_valid_contiguous = np.ascontiguousarray(vaid_x)\n",
    "    y_pred = clf.predict(X_valid_contiguous)\n",
    "    print(f\"Accuracy Score of SVM with polynomial kernal function with degree 3 \", accuracy_score(valid_y, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8872dc3",
   "metadata": {},
   "source": [
    "Logistic Regression Model\n",
    "Checking Hyper Parameter tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2215bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def try_logistic(train_x, train_y, vaid_x, valid_y):\n",
    "    for i in [\"lbfgs\",\"newton-cg\",\"sag\",\"saga\"]:\n",
    "        logistic_regression = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "        logistic_regression.fit(train_x,train_y)\n",
    "        y_pred = logistic_regression.predict(vaid_x)\n",
    "        print(f\"Accuracy Score of Logistic Regression with solver {i}\", accuracy_score(valid_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c5c39",
   "metadata": {},
   "source": [
    "Naive Bayes \n",
    "Trying out different distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2c6bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "def try_naive(train_x, train_y, vaid_x, valid_y):\n",
    "    \n",
    "    min_value = np.min(train_x)\n",
    "    scaled_train_x = train_x - min_value\n",
    "    scaled_vaid_x = vaid_x - min_value\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(scaled_train_x, train_y)\n",
    "    y_pred = clf.predict(scaled_vaid_x)\n",
    "    print(f\"Accuracy Score of Naive Bayes with Gaussian Naive Bayes\", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(scaled_train_x, train_y)\n",
    "    y_pred = clf.predict(vaid_x)\n",
    "    print(f\"Accuracy Score of Naive Bayes with MultinomialNB\", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = ComplementNB()\n",
    "    clf.fit(scaled_train_x, train_y)\n",
    "    y_pred = clf.predict(scaled_vaid_x)\n",
    "    print(f\"Accuracy Score of Naive Bayes with ComplementNB\", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = CategoricalNB()\n",
    "    clf.fit(scaled_train_x, train_y)\n",
    "    y_pred = clf.predict(scaled_vaid_x)\n",
    "    print(f\"Accuracy Score of Naive Bayes with CategoricalNB\", accuracy_score(valid_y, y_pred))\n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "    clf.fit(scaled_train_x, train_y)\n",
    "    y_pred = clf.predict(scaled_vaid_x)\n",
    "    print(f\"Accuracy Score of Naive Bayes with BernoulliNB\", accuracy_score(valid_y, y_pred))  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f683f8",
   "metadata": {},
   "source": [
    "DecisionTree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e2b570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def try_decisionTree(train_x, train_y, vaid_x, valid_y):\n",
    "    # Initialize and train the Decision Tree classifier\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(train_x, train_y)\n",
    "    y_pred = clf.predict(vaid_x)\n",
    "    print(f\"Accuracy Score of Decision Tree\", accuracy_score(valid_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43829835",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c869ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def try_random_forest(train_x, train_y, vaid_x, valid_y):\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(train_x, train_y)\n",
    "    y_pred = clf.predict(vaid_x)\n",
    "    print(f\"Accuracy Score of Random Forest\", accuracy_score(valid_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236af1df",
   "metadata": {},
   "source": [
    "XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f65aa6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.0-py3-none-win_amd64.whl (99.7 MB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#installing XG Boost\n",
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bf6e9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def try_xgBoost(train_x, train_y, vaid_x, valid_y):\n",
    "    # Create an XGBoost classifier for multi-class classification\n",
    "    clf = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # Set the objective for multi-class classification\n",
    "    num_class=len(np.unique(y)),  # Number of classes\n",
    "    random_state=42\n",
    "    )\n",
    "    le = LabelEncoder()\n",
    "    train_y = le.fit_transform(train_y)\n",
    "    clf.fit(train_x, train_y)\n",
    "    y_pred = clf.predict(vaid_x)\n",
    "    y_pred = le.inverse_transform(y_pred)\n",
    "    print(f\"Accuracy Score of XG Boost\", accuracy_score(valid_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9200395",
   "metadata": {},
   "source": [
    "# Feature Engineering Using Mutual Info Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa2e7ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "def select_cols_using_mutual_info_regression(x,y,n) :\n",
    "    selected_columns = SelectKBest(mutual_info_classif, k=n)\n",
    "    selected_columns.fit(x, y)\n",
    "    return x.columns[selected_columns.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3fd93ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_cols = select_cols_using_mutual_info_regression(x,y['label_1'],300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e54ed027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After reducing rows, I got poor validation accuracy, So I decided to not to use this method.\n",
    "#x = x[selected_cols]\n",
    "#x_valid = x_valid[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26bd1b",
   "metadata": {},
   "source": [
    "# Label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d70d5f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best Hyperparameters: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "Accuracy for KNN 0.9053333333333333\n"
     ]
    }
   ],
   "source": [
    "#Doing Hyper parameter tuning for KNN & checking the best accuracy for label 1\n",
    "try_knn(x,y[\"label_1\"],x_valid,y_valid[\"label_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "24cb0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of SVM with default settings (RBF - exponential kernal)  0.9346666666666666\n",
      "Accuracy Score of Linear SVM (one-vs-the-rest)  0.9706666666666667\n",
      "Accuracy Score of SVM with linear kernal function  0.9506666666666667\n",
      "Accuracy Score of SVM with sigmoid kernal function  0.8426666666666667\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 2  0.92\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 3  0.92\n"
     ]
    }
   ],
   "source": [
    "#Checking SVM Values for Label 1\n",
    "try_svm(x,y[\"label_1\"],x_valid,y_valid[\"label_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ae1eabf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Logistic Regression with solver lbfgs 0.9546666666666667\n",
      "Accuracy Score of Logistic Regression with solver newton-cg 0.9546666666666667\n",
      "Accuracy Score of Logistic Regression with solver sag 0.9546666666666667\n",
      "Accuracy Score of Logistic Regression with solver saga 0.9546666666666667\n"
     ]
    }
   ],
   "source": [
    "#Checking Logistic Regression Values for Label1. It provided very good results. \n",
    "try_logistic(x,y[\"label_1\"],x_valid,y_valid[\"label_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4fa2fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Naive Bayes with Gaussian Naive Bayes 0.62\n",
      "Accuracy Score of Naive Bayes with MultinomialNB 0.17333333333333334\n",
      "Accuracy Score of Naive Bayes with ComplementNB 0.376\n",
      "Accuracy Score of Naive Bayes with CategoricalNB 0.204\n",
      "Accuracy Score of Naive Bayes with BernoulliNB 0.013333333333333334\n"
     ]
    }
   ],
   "source": [
    "#It gave bad results\n",
    "try_naive(x,y[\"label_1\"],x_valid,y_valid[\"label_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "16354f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Decision Tree 0.32666666666666666\n"
     ]
    }
   ],
   "source": [
    "try_decisionTree(x,y[\"label_1\"],x_valid,y_valid[\"label_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1fa9bdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Random Forest 0.8706666666666667\n"
     ]
    }
   ],
   "source": [
    "try_random_forest(x,y[\"label_1\"],x_valid,y_valid[\"label_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c6a17cee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28364/3753612166.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtry_xgBoost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28364/4083368074.py\u001b[0m in \u001b[0;36mtry_xgBoost\u001b[1;34m(train_x, train_y, vaid_x, valid_y)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvaid_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1513\u001b[0m             )\n\u001b[0;32m   1514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1515\u001b[1;33m             self._Booster = train(\n\u001b[0m\u001b[0;32m   1516\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m             _check_call(\n\u001b[1;32m-> 2050\u001b[1;33m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   2051\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m                 )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try_xgBoost(x,y[\"label_1\"],x_valid,y_valid[\"label_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b321d",
   "metadata": {},
   "source": [
    "When considering validation accuracy results of label 1, Linear SVM and Logistic Regression gave good results, We can use Linear SVM model for our test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9dcac",
   "metadata": {},
   "source": [
    "# Label 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "326aafd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n",
      "Best Hyperparameters: {'n_neighbors': 15, 'weights': 'distance'}\n",
      "Accuracy for KNN 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "try_knn(x,y[\"label_3\"],x_valid,y_valid[\"label_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "488dbf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Logistic Regression with solver lbfgs 0.9973333333333333\n",
      "Accuracy Score of Logistic Regression with solver newton-cg 0.9973333333333333\n",
      "Accuracy Score of Logistic Regression with solver sag 0.9973333333333333\n",
      "Accuracy Score of Logistic Regression with solver saga 0.9973333333333333\n"
     ]
    }
   ],
   "source": [
    "try_logistic(x,y[\"label_3\"],x_valid,y_valid[\"label_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8d316e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of SVM with default settings (RBF - exponential kernal)  0.9986666666666667\n",
      "Accuracy Score of Linear SVM (one-vs-the-rest)  0.9973333333333333\n",
      "Accuracy Score of SVM with linear kernal function  0.9973333333333333\n",
      "Accuracy Score of SVM with sigmoid kernal function  0.9586666666666667\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 2  0.9933333333333333\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 3  0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "try_svm(x,y[\"label_3\"],x_valid,y_valid[\"label_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3726f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Naive Bayes with Gaussian Naive Bayes 0.5786666666666667\n",
      "Accuracy Score of Naive Bayes with MultinomialNB 0.22666666666666666\n",
      "Accuracy Score of Naive Bayes with ComplementNB 0.41733333333333333\n",
      "Accuracy Score of Naive Bayes with CategoricalNB 0.20266666666666666\n",
      "Accuracy Score of Naive Bayes with BernoulliNB 0.016\n"
     ]
    }
   ],
   "source": [
    "try_naive(x,y[\"label_3\"],x_valid,y_valid[\"label_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "90c0140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Decision Tree 0.9106666666666666\n"
     ]
    }
   ],
   "source": [
    "try_decisionTree(x,y[\"label_3\"],x_valid,y_valid[\"label_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1013f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Random Forest 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "try_random_forest(x,y[\"label_3\"],x_valid,y_valid[\"label_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ce9af867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of XG Boost 0.9853333333333333\n"
     ]
    }
   ],
   "source": [
    "try_xgBoost(x,y[\"label_3\"],x_valid,y_valid[\"label_3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96907411",
   "metadata": {},
   "source": [
    "When considering validation accuracy results of label 3, KNN, Logistic Regression, SVM and XG Boost provided good accuracy. Since we got good accuracy close to 100%, We can use SVM model for our test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a220f10f",
   "metadata": {},
   "source": [
    "# Label 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5633b",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance using SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65f0d5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     19938\n",
       "2      1449\n",
       "0       955\n",
       "12      954\n",
       "7       938\n",
       "13      482\n",
       "1       481\n",
       "11      480\n",
       "10      480\n",
       "3       479\n",
       "5       478\n",
       "9       472\n",
       "4       469\n",
       "8       465\n",
       "Name: label_4, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[\"label_4\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ceb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27101eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since 6 value affects class balance property. We decided to undersample it.\n",
    "from imblearn.over_sampling  import SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(x, y[\"label_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "93648585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6     19938\n",
       "13    19938\n",
       "4     19938\n",
       "5     19938\n",
       "1     19938\n",
       "2     19938\n",
       "7     19938\n",
       "3     19938\n",
       "0     19938\n",
       "12    19938\n",
       "9     19938\n",
       "8     19938\n",
       "11    19938\n",
       "10    19938\n",
       "Name: label_4, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts()\n",
    "#However resampled values gave poor performance. So I avoided using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ac479bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best Hyperparameters: {'n_neighbors': 7, 'weights': 'distance'}\n",
      "Accuracy for KNN 0.948\n"
     ]
    }
   ],
   "source": [
    "try_knn(x,y[\"label_4\"],x_valid,y_valid[\"label_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cdac4be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Logistic Regression with solver lbfgs 0.924\n",
      "Accuracy Score of Logistic Regression with solver newton-cg 0.924\n",
      "Accuracy Score of Logistic Regression with solver sag 0.924\n",
      "Accuracy Score of Logistic Regression with solver saga 0.924\n"
     ]
    }
   ],
   "source": [
    "try_logistic(x,y[\"label_4\"],x_valid,y_valid[\"label_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b060fc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of SVM with default settings (RBF - exponential kernal)  0.9346666666666666\n",
      "Accuracy Score of Linear SVM (one-vs-the-rest)  0.944\n",
      "Accuracy Score of SVM with linear kernal function  0.9426666666666667\n",
      "Accuracy Score of SVM with sigmoid kernal function  0.768\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 2  0.9293333333333333\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 3  0.9293333333333333\n"
     ]
    }
   ],
   "source": [
    "try_svm(x,y[\"label_4\"],x_valid,y_valid[\"label_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7c3ea74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Naive Bayes with Gaussian Naive Bayes 0.39066666666666666\n",
      "Accuracy Score of Naive Bayes with MultinomialNB 0.7093333333333334\n",
      "Accuracy Score of Naive Bayes with ComplementNB 0.5466666666666666\n",
      "Accuracy Score of Naive Bayes with CategoricalNB 0.6826666666666666\n",
      "Accuracy Score of Naive Bayes with BernoulliNB 0.7053333333333334\n"
     ]
    }
   ],
   "source": [
    "try_naive(x,y[\"label_4\"],x_valid,y_valid[\"label_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a90ff94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Decision Tree 0.672\n"
     ]
    }
   ],
   "source": [
    "try_decisionTree(x,y[\"label_4\"],x_valid,y_valid[\"label_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "06ec5159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Random Forest 0.7733333333333333\n"
     ]
    }
   ],
   "source": [
    "try_random_forest(x,y[\"label_4\"],x_valid,y_valid[\"label_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "91d05d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of XG Boost 0.824\n"
     ]
    }
   ],
   "source": [
    "try_xgBoost(x,y[\"label_4\"],x_valid,y_valid[\"label_4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c664d16",
   "metadata": {},
   "source": [
    "When considering validation accuracy results of label 4, KNN, Logistic Regression and SVM  provided good accuracy. We can use ensambled method consisting above classifiers to predict values for our test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e274b3a",
   "metadata": {},
   "source": [
    "# Label 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687526f8",
   "metadata": {},
   "source": [
    "# Handling Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ea3af",
   "metadata": {},
   "source": [
    "Since i am handling each feature separately, I thought removing the rows which have empty values for label 2, is the best way to handle the missing values for label 2 as no artifical entries will not be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fbcb3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label_2 = train.dropna(subset=['label_2'])\n",
    "valid_label_2 = valid.dropna(subset=['label_2'])\n",
    "y_label_2 = data_label_2[[\"label_1\",\"label_2\",\"label_3\",\"label_4\"]] \n",
    "x_label_2 = data_label_2.drop(y_label_2, axis=1)\n",
    "y_label_2_valid = valid_label_2[[\"label_1\",\"label_2\",\"label_3\",\"label_4\"]] \n",
    "x_label_2_valid = valid_label_2.drop(y_label_2_valid, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9169c110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Best Hyperparameters: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "Accuracy for KNN 0.9144021739130435\n"
     ]
    }
   ],
   "source": [
    "try_knn(x_label_2,y_label_2[\"label_2\"],x_label_2_valid,y_label_2_valid[\"label_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "45446d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Logistic Regression with solver lbfgs 0.7377717391304348\n",
      "Accuracy Score of Logistic Regression with solver newton-cg 0.7377717391304348\n",
      "Accuracy Score of Logistic Regression with solver sag 0.7377717391304348\n",
      "Accuracy Score of Logistic Regression with solver saga 0.7377717391304348\n"
     ]
    }
   ],
   "source": [
    "try_logistic(x_label_2,y_label_2[\"label_2\"],x_label_2_valid,y_label_2_valid[\"label_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "086171f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of SVM with default settings (RBF - exponential kernal)  0.875\n",
      "Accuracy Score of Linear SVM (one-vs-the-rest)  0.8029891304347826\n",
      "Accuracy Score of SVM with linear kernal function  0.8125\n",
      "Accuracy Score of SVM with sigmoid kernal function  0.4320652173913043\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 2  0.8491847826086957\n",
      "Accuracy Score of SVM with polynomial kernal function with degree 3  0.8491847826086957\n"
     ]
    }
   ],
   "source": [
    "try_svm(x_label_2,y_label_2[\"label_2\"],x_label_2_valid,y_label_2_valid[\"label_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "55ab1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Naive Bayes with Gaussian Naive Bayes 0.3451086956521739\n",
      "Accuracy Score of Naive Bayes with MultinomialNB 0.20516304347826086\n",
      "Accuracy Score of Naive Bayes with ComplementNB 0.30027173913043476\n",
      "Accuracy Score of Naive Bayes with CategoricalNB 0.22554347826086957\n",
      "Accuracy Score of Naive Bayes with BernoulliNB 0.16304347826086957\n"
     ]
    }
   ],
   "source": [
    "try_naive(x_label_2,y_label_2[\"label_2\"],x_label_2_valid,y_label_2_valid[\"label_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3a46eb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Decision Tree 0.38179347826086957\n"
     ]
    }
   ],
   "source": [
    "try_decisionTree(x_label_2,y_label_2[\"label_2\"],x_label_2_valid,y_label_2_valid[\"label_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2b7dcb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Random Forest 0.7472826086956522\n"
     ]
    }
   ],
   "source": [
    "try_random_forest(x_label_2,y_label_2[\"label_2\"],x_label_2_valid,y_label_2_valid[\"label_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2a57f778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of XG Boost 0.8654891304347826\n"
     ]
    }
   ],
   "source": [
    "try_xgBoost(x_label_2,y_label_2[\"label_2\"],x_label_2_valid,y_label_2_valid[\"label_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8af0c",
   "metadata": {},
   "source": [
    "Here KNN model performed well beating other models. So here we can use KNN model to predict label 2 data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a155629",
   "metadata": {},
   "source": [
    "# Evaluating test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d12768a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "84b3e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(\"ID\", axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dff209d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_1      0\n",
       "feature_2      0\n",
       "feature_3      0\n",
       "feature_4      0\n",
       "feature_5      0\n",
       "              ..\n",
       "feature_764    0\n",
       "feature_765    0\n",
       "feature_766    0\n",
       "feature_767    0\n",
       "feature_768    0\n",
       "Length: 768, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking provided data\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33659d2",
   "metadata": {},
   "source": [
    "# Label 1 - Linear SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "973dd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC(dual=\"auto\")\n",
    "clf.fit(x, y[\"label_1\"])\n",
    "X_test_contiguous = np.ascontiguousarray(test)\n",
    "y_pred_label_1 = clf.predict(X_test_contiguous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ef2dc",
   "metadata": {},
   "source": [
    "# Label 3 - SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d77ec561",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(x, y[\"label_3\"])\n",
    "y_pred_label_3 = clf.predict(X_test_contiguous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed957a0",
   "metadata": {},
   "source": [
    "# Label 2 - KNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "561fb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3  # Number of neighbors\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_model.fit(x_label_2, y_label_2[\"label_2\"])\n",
    "y_pred_label_2 = knn_model.predict(X_test_contiguous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d101c62",
   "metadata": {},
   "source": [
    "# Label 4 - Ensamble Method of SVM, Logistic Regression, KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3fdd4",
   "metadata": {},
   "source": [
    "Checking Using Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d95200b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.944\n",
      "Logistic Regression Accuracy: 0.924\n",
      "KNN Accuracy: 0.944\n",
      "Ensemble Accuracy: 0.9493333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Initialize individual models\n",
    "svm_model = svm.LinearSVC(dual=\"auto\")\n",
    "logistic_model = LogisticRegression()\n",
    "knn_model = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# Train individual models\n",
    "svm_model.fit(x, y[\"label_4\"])\n",
    "logistic_model.fit(x, y[\"label_4\"])\n",
    "knn_model.fit(x, y[\"label_4\"])\n",
    "\n",
    "# Make predictions on the validation set\n",
    "X_valid_contiguous = np.ascontiguousarray(x_valid)\n",
    "svm_pred = svm_model.predict(X_valid_contiguous)\n",
    "logistic_pred = logistic_model.predict(x_valid)\n",
    "knn_pred = knn_model.predict(X_valid_contiguous)\n",
    "\n",
    "# Create an ensemble using a VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('svm', svm_model),\n",
    "    ('logistic', logistic_model),\n",
    "    ('knn', knn_model)\n",
    "], voting='hard')  # 'hard' for majority voting, 'soft' for weighted voting based on class probabilities\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble.fit(x, y[\"label_4\"])\n",
    "\n",
    "# Make predictions using the ensemble\n",
    "ensemble_pred = ensemble.predict(X_valid_contiguous)\n",
    "\n",
    "# Evaluate individual models\n",
    "svm_accuracy = accuracy_score(y_valid[\"label_4\"], svm_pred)\n",
    "logistic_accuracy = accuracy_score(y_valid[\"label_4\"], logistic_pred)\n",
    "knn_accuracy = accuracy_score(y_valid[\"label_4\"], knn_pred)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "ensemble_accuracy = accuracy_score(y_valid[\"label_4\"], ensemble_pred)\n",
    "\n",
    "print(f'SVM Accuracy: {svm_accuracy}')\n",
    "print(f'Logistic Regression Accuracy: {logistic_accuracy}')\n",
    "print(f'KNN Accuracy: {knn_accuracy}')\n",
    "print(f'Ensemble Accuracy: {ensemble_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149f0ee",
   "metadata": {},
   "source": [
    "Since Ensamble accuracy is greater than individual accuracy, I decide to use this method for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7cb4b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_label_4 = knn_model.predict(X_test_contiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7ca5f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([ pd.Series(y_pred_label_1,name='label_1'), pd.Series(y_pred_label_2,name='label_2'), pd.Series(y_pred_label_3,name='label_3'), pd.Series(y_pred_label_4,name='label_4')], axis=1)\n",
    "\n",
    "# Reset the index and add an incrementing ID column\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df['ID'] = combined_df.index + 1\n",
    "\n",
    "# Reorder the columns with 'ID' as the leftmost column\n",
    "combined_df = combined_df[['ID'] + [col for col in combined_df.columns if col != 'ID']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f378c93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>740</td>\n",
       "      <td>46</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>741</td>\n",
       "      <td>35</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>742</td>\n",
       "      <td>54</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>743</td>\n",
       "      <td>38</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>744</td>\n",
       "      <td>51</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  label_1  label_2  label_3  label_4\n",
       "0      1       26     22.0        0        2\n",
       "1      2       18     25.0        1        8\n",
       "2      3       16     30.0        1        6\n",
       "3      4        7     27.0        1        6\n",
       "4      5       58     29.0        0        6\n",
       "..   ...      ...      ...      ...      ...\n",
       "739  740       46     24.0        1        2\n",
       "740  741       35     24.0        1        2\n",
       "741  742       54     27.0        1        6\n",
       "742  743       38     32.0        1       12\n",
       "743  744       51     26.0        1        6\n",
       "\n",
       "[744 rows x 5 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "91123d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the CSV file\n",
    "file_path = \"190199A_layer_8.csv\"  # Replace 'your_file_name.csv' with the desired file name and path\n",
    "\n",
    "# Use the to_csv method to save the DataFrame to a CSV file\n",
    "combined_df.to_csv(file_path, index=False)  # Set index to False if you don't want to save the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9297015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
